# -*- coding: utf-8 -*-
"""TJBioInfoHackathon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BoXUsdICRSV2idv9051po1e-4CexuaaS
"""

#Author: Neil Daniel
#Date: 2/16/22

import pandas as pd
import sklearn
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
import warnings
#Ignore any warnings that may arise
warnings.simplefilter('ignore', UserWarning)


train = pd.read_csv("HCV-Egy-Data.csv")
X = train.drop(columns=['Label'])
y = train['Baselinehistological staging']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15)
print(X_train)

#Choose between DecisionTreeClasifier, Nearest Neighbors, SVC, other models

test_accuracies = []
for n in range(1,30):
  knn = KNeighborsClassifier(n_neighbors=n)
  knn.fit(X_train,y_train)
  expectedKNN = knn.predict(X_test)
  KNNscore = accuracy_score(y_test, expectedKNN)
  test_accuracies.append(KNNscore)

knnMax = (max(test_accuracies))
print('KNN Accuracy: '+str(knnMax * 100) + " percent")

model = DecisionTreeClassifier()
model.fit(X_train, y_train)
expectedDT = model.predict(X_test)
DTscore = accuracy_score(y_test, expectedDT)
print('DT Accuracy: '+str(DTscore * 100) + " percent")

svm = SVC(C=1.0)
svm.fit(X_train,y_train)
expectedSVM = svm.predict(X_test)
SVMscore = accuracy_score(y_test, expectedSVM)
print('SVM Accuracy: '+str(SVMscore * 100) + " percent")

log = LogisticRegression(solver='lbfgs')
log.fit(X_train, y_train)
expectedLOG = log.predict(X_test)
LOGscore = accuracy_score(y_test, expectedLOG)
print('LOG Accuracy: '+str(LOGscore * 100) + " percent")


#Accuracy is very low -> There is probably no correlation but let's make sure the model works by seeing if the correct value is returned for the first row of datum.

x = model.predict([[56, 1, 35, 2, 1, 1, 1, 2, 2, 2, 7425, 4248807, 14, 112132, 99, 84, 52, 109, 81, 5, 5, 5, 655330, 634536, 288194, 5, 5, 13]])
print(x) #Yes, the model most likely works.

#We could try more models, but it is obvious there is no correlation here

bestModel = max(knnMax, DTscore, SVMscore)

print("") #For spacing

if(bestModel == knnMax):
  print("The best performing model this run is KNN with accuracy: " + str(bestModel * 100) + " percent")

elif(bestModel == DTscore):
  print("The best performing model this run is DT with accuracy: " + str(bestModel * 100) + " percent")

elif(bestModel == SVMscore):
  print("The best performing model this run is SVM with accuracy: " + str(bestModel * 100) + " percent")

else:
  print("There is a tie and the accuracy is: " + str(bestModel * 100) + " percent")

print("") #For spacing

#CONCLUSION

toRet = """After repeated trials of the dataset,
it can be determined k-nearest neighbors, decision trees,
and support vector machines all perform poorly. They achieve accuracies between
21 and 30 percent upon multiple trials, suggesting there is no correlation between
columns 1 - 27 as a predictor of Baselinehistological staging. Since the accuracies
hover around 25 percent and there are 4 possibilities for the desired column, these models
are essentially no more correct than a guess at random. """

print(toRet)